{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distribute hard-to-classify datapoints over CV folds\n\n'Instance hardness' refers to the difficulty to classify an instance. The way\nhard-to-classify instances are distributed over train and test sets has\nsignificant effect on the test set performance metrics. In this example we\nshow how to deal with this problem. We are making the comparison with normal\n:class:`~sklearn.model_selection.StratifiedKFold` cross-validation splitter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Frits Hermans, https://fritshermans.github.io\n# License: MIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create an imbalanced dataset with instance hardness\n\nWe create an imbalanced dataset with using scikit-learn's\n:func:`~sklearn.datasets.make_blobs` function and set the class imbalance ratio to\n5%.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=[950, 50], centers=((-3, 0), (3, 0)), random_state=10)\n_ = plt.scatter(X[:, 0], X[:, 1], c=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To introduce instance hardness in our dataset, we add some hard to classify samples:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_hard, y_hard = make_blobs(\n    n_samples=10, centers=((3, 0), (-3, 0)), cluster_std=1, random_state=10\n)\nX, y = np.vstack((X, X_hard)), np.hstack((y, y_hard))\n_ = plt.scatter(X[:, 0], X[:, 1], c=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare cross validation scores using `StratifiedKFold` and `InstanceHardnessCV`\n\nNow, we want to assess a linear predictive model. Therefore, we should use\ncross-validation. The most important concept with cross-validation is to create\ntraining and test splits that are representative of the the data in production to have\nstatistical results that one can expect in production.\n\nBy applying a standard :class:`~sklearn.model_selection.StratifiedKFold`\ncross-validation splitter, we do not control in which fold the hard-to-classify\nsamples will be.\n\nThe :class:`~imblearn.model_selection.InstanceHardnessCV` splitter allows to\ncontrol the distribution of the hard-to-classify samples over the folds.\n\nLet's make an experiment to compare the results that we get with both splitters.\nWe use a :class:`~sklearn.linear_model.LogisticRegression` classifier and\n:func:`~sklearn.model_selection.cross_validate` to calculate the cross validation\nscores. We use average precision for scoring.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\n\nfrom imblearn.model_selection import InstanceHardnessCV\n\nlogistic_regression = LogisticRegression()\n\nresults = {}\nfor cv in (\n    StratifiedKFold(n_splits=5, shuffle=True, random_state=10),\n    InstanceHardnessCV(estimator=LogisticRegression()),\n):\n    result = cross_validate(\n        logistic_regression,\n        X,\n        y,\n        cv=cv,\n        scoring=\"average_precision\",\n    )\n    results[cv.__class__.__name__] = result[\"test_score\"]\nresults = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = results.plot.box(vert=False, whis=[0, 100])\n_ = ax.set(\n    xlabel=\"Average precision\",\n    title=\"Cross validation scores with different splitters\",\n    xlim=(0, 1),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The boxplot shows that the :class:`~imblearn.model_selection.InstanceHardnessCV`\nsplitter results in less variation of average precision than\n:class:`~sklearn.model_selection.StratifiedKFold` splitter. When doing\nhyperparameter tuning or feature selection using a wrapper method (like\n:class:`~sklearn.feature_selection.RFECV`) this will give more stable results.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}